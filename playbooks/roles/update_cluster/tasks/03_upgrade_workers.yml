---
# Upgrade Worker Nodes with Rolling Update Strategy

- name: Find home directory for UID 1000 (on master node for kubectl commands)
  command: getent passwd 1000
  register: getent_passwd_output
  changed_when: false
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true

- name: Set home directory variable (for all workers)
  set_fact:
    user_home_directory: "{{ getent_passwd_output.stdout.split(':')[5] }}"
  when: getent_passwd_output.stdout != ""

- name: Define the user name with UID 1000
  command: getent passwd 1000
  register: getent_passwd_result
  changed_when: false

- name: Set user name variable from getent passwd output
  set_fact:
    uid_1000_user: "{{ getent_passwd_result.stdout.split(':')[0] }}"
  when: getent_passwd_result.stdout != ''

- name: Get K3s token from first master (for workers to join)
  shell: cat /var/lib/rancher/k3s/server/node-token
  register: k3s_token_raw
  become: true
  changed_when: false
  run_once: true
  delegate_to: "{{ groups['master'][0] }}"

- name: Set K3s token fact (for all workers)
  set_fact:
    k3s_token: "{{ k3s_token_raw.stdout | default('') }}"
  when: k3s_token_raw.stdout is defined

- name: Get keepalived VIP from vars
  set_fact:
    keepalived_vip: "{{ keepalived_vip | default('192.168.1.244') }}"
  run_once: true

- name: Count master nodes
  set_fact:
    master_count: "{{ groups['master'] | length | int }}"
  run_once: true

- name: Upgrade worker node (HA mode)
  block:
    - name: Check pods on worker node before drain
      ansible.builtin.shell:
        cmd: kubectl get pods --all-namespaces --field-selector spec.nodeName={{ inventory_hostname }} -o wide --no-headers | wc -l
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      delegate_to: "{{ groups['master'][0] }}"
      register: pods_count_before
      changed_when: false

    - name: Check PVC pods on worker node before drain
      ansible.builtin.shell:
        cmd: kubectl get pods --all-namespaces --field-selector spec.nodeName={{ inventory_hostname }} -o jsonpath='{range .items[*]}{.metadata.namespace}{\"/\"}{.metadata.name}{\" \"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{\" \"}{end}{\"\\n\"}{end}' | grep -v '^$' || echo "No PVC pods"
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      delegate_to: "{{ groups['master'][0] }}"
      register: pvc_pods_before
      changed_when: false
      failed_when: false

    - name: Check Longhorn pods on worker node before drain
      ansible.builtin.shell:
        cmd: kubectl get pods -n longhorn-system --field-selector spec.nodeName={{ inventory_hostname }} -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}' || echo ""
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      delegate_to: "{{ groups['master'][0] }}"
      register: longhorn_pods_before
      changed_when: false
      failed_when: false

    - name: Display worker node status before drain
      ansible.builtin.debug:
        msg: |
          Worker node {{ inventory_hostname }} status before drain:
          - Total pods: {{ pods_count_before.stdout | trim }}
          - PVC pods: {{ pvc_pods_before.stdout | default('None') }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3

    - name: Drain worker node
      ansible.builtin.command:
        cmd: kubectl drain {{ inventory_hostname }} --ignore-daemonsets --delete-emptydir-data --timeout={{ upgrade_drain_timeout }}s --grace-period={{ upgrade_drain_grace_period | default(60) }} --force
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      delegate_to: "{{ groups['master'][0] }}"
      register: drain_worker_result
      failed_when: false
      changed_when: drain_worker_result.rc == 0

    - name: Display drain output for debugging
      ansible.builtin.debug:
        msg: |
          Drain output for {{ inventory_hostname }}:
          STDOUT: {{ drain_worker_result.stdout | default('None') }}
          STDERR: {{ drain_worker_result.stderr | default('None') }}
          Return code: {{ drain_worker_result.rc }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3

    - name: Check pods that could not be evicted
      ansible.builtin.shell:
        cmd: kubectl get pods --all-namespaces --field-selector spec.nodeName={{ inventory_hostname }} --no-headers 2>/dev/null | awk '{print $1"/"$2}' || echo "No pods remaining"
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
        - drain_worker_result.rc != 0
      delegate_to: "{{ groups['master'][0] }}"
      register: remaining_pods
      changed_when: false
      failed_when: false

    - name: Display pods that could not be evicted
      ansible.builtin.debug:
        msg: |
          Pods that could not be evicted from {{ inventory_hostname }}:
          {{ remaining_pods.stdout | default('None') }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
        - drain_worker_result.rc != 0
        - remaining_pods.stdout is defined

    - name: Check if drain was successful
      ansible.builtin.debug:
        msg: "Drain completed successfully for {{ inventory_hostname }}"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
        - drain_worker_result.rc == 0

    - name: Warn if drain had issues (may continue with force)
      ansible.builtin.debug:
        msg: "Drain had issues for {{ inventory_hostname }}, but continuing with upgrade (some pods may not have been evicted)"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
        - drain_worker_result.rc != 0

    - name: Upgrade K3s agent on worker (HA mode)
      shell: |
        export K3S_URL="https://{{ keepalived_vip }}:6443"
        export K3S_TOKEN="{{ k3s_token }}"
        export INSTALL_K3S_VERSION={{ k3s_target_version }}
        curl -sfL {{ k3s_install_url }} | sh -s - agent
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      become: true
      register: upgrade_worker_ha

    - name: Wait for K3s agent to be ready (HA mode)
      ansible.builtin.wait_for:
        path: /var/lib/rancher/k3s/agent/node-token
        state: present
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      become: true
      retries: 10
      delay: 10

    - name: Wait for Longhorn pods to recover after upgrade
      ansible.builtin.shell:
        cmd: |
          # Wait for Longhorn to recover replica pods (max 2 minutes)
          for i in {1..12}; do
            failed_replicas=$(kubectl get pods -n longhorn-system --field-selector spec.nodeName={{ inventory_hostname }} -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' | grep -c "Failed" || echo "0")
            if [ "$failed_replicas" -eq "0" ]; then
              echo "Longhorn pods recovered"
              exit 0
            fi
            sleep 10
          done
          echo "Warning: Some Longhorn pods may still be recovering"
          exit 0
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
        - longhorn_pods_before.stdout is defined
        - longhorn_pods_before.stdout != ""
      delegate_to: "{{ groups['master'][0] }}"
      register: longhorn_recovery
      changed_when: false
      failed_when: false

    - name: Display Longhorn recovery status
      ansible.builtin.debug:
        msg: |
          Longhorn recovery status for {{ inventory_hostname }}:
          {{ longhorn_recovery.stdout | default('No Longhorn pods found before drain') }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
        - longhorn_recovery.stdout is defined

    - name: Uncordon worker node (HA mode)
      ansible.builtin.command:
        cmd: kubectl uncordon {{ inventory_hostname }}
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3
      delegate_to: "{{ groups['master'][0] }}"

    - name: Wait for pods to stabilize after worker upgrade (HA mode)
      ansible.builtin.pause:
        seconds: "{{ upgrade_wait_for_pods }}"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) >= 3

  when: 
    - "'worker' in group_names"
    - upgrade_needed | default(true)
    - (master_count | int) >= 3

- name: Upgrade worker node (Single Master mode)
  block:
    - name: Check pods on worker node before drain (Single Master)
      ansible.builtin.shell:
        cmd: kubectl get pods --all-namespaces --field-selector spec.nodeName={{ inventory_hostname }} -o wide --no-headers | wc -l
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      delegate_to: "{{ groups['master'][0] }}"
      register: pods_count_before_single
      changed_when: false

    - name: Check PVC pods on worker node before drain (Single Master)
      ansible.builtin.shell:
        cmd: kubectl get pods --all-namespaces --field-selector spec.nodeName={{ inventory_hostname }} -o jsonpath='{range .items[*]}{.metadata.namespace}{\"/\"}{.metadata.name}{\" \"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{\" \"}{end}{\"\\n\"}{end}' | grep -v '^$' || echo "No PVC pods"
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      delegate_to: "{{ groups['master'][0] }}"
      register: pvc_pods_before_single
      changed_when: false
      failed_when: false

    - name: Check Longhorn pods on worker node before drain (Single Master)
      ansible.builtin.shell:
        cmd: kubectl get pods -n longhorn-system --field-selector spec.nodeName={{ inventory_hostname }} -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}' || echo ""
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      delegate_to: "{{ groups['master'][0] }}"
      register: longhorn_pods_before_single
      changed_when: false
      failed_when: false

    - name: Display worker node status before drain (Single Master)
      ansible.builtin.debug:
        msg: |
          Worker node {{ inventory_hostname }} status before drain:
          - Total pods: {{ pods_count_before_single.stdout | trim }}
          - PVC pods: {{ pvc_pods_before_single.stdout | default('None') }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3

    - name: Drain worker node
      ansible.builtin.command:
        cmd: kubectl drain {{ inventory_hostname }} --ignore-daemonsets --delete-emptydir-data --timeout={{ upgrade_drain_timeout }}s --grace-period={{ upgrade_drain_grace_period | default(60) }} --force
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      delegate_to: "{{ groups['master'][0] }}"
      register: drain_worker_result_single
      failed_when: false
      changed_when: drain_worker_result_single.rc == 0

    - name: Display drain output for debugging (Single Master)
      ansible.builtin.debug:
        msg: |
          Drain output for {{ inventory_hostname }}:
          STDOUT: {{ drain_worker_result_single.stdout | default('None') }}
          STDERR: {{ drain_worker_result_single.stderr | default('None') }}
          Return code: {{ drain_worker_result_single.rc }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3

    - name: Check pods that could not be evicted (Single Master)
      ansible.builtin.shell:
        cmd: kubectl get pods --all-namespaces --field-selector spec.nodeName={{ inventory_hostname }} --no-headers 2>/dev/null | awk '{print $1"/"$2}' || echo "No pods remaining"
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
        - drain_worker_result_single.rc != 0
      delegate_to: "{{ groups['master'][0] }}"
      register: remaining_pods_single
      changed_when: false
      failed_when: false

    - name: Display pods that could not be evicted (Single Master)
      ansible.builtin.debug:
        msg: |
          Pods that could not be evicted from {{ inventory_hostname }}:
          {{ remaining_pods_single.stdout | default('None') }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
        - drain_worker_result_single.rc != 0
        - remaining_pods_single.stdout is defined

    - name: Check if drain was successful (Single Master)
      ansible.builtin.debug:
        msg: "Drain completed successfully for {{ inventory_hostname }}"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
        - drain_worker_result_single.rc == 0

    - name: Warn if drain had issues (Single Master, may continue with force)
      ansible.builtin.debug:
        msg: "Drain had issues for {{ inventory_hostname }}, but continuing with upgrade (some pods may not have been evicted)"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
        - drain_worker_result_single.rc != 0

    - name: Upgrade K3s agent on worker (Single Master mode)
      shell: |
        export K3S_URL="https://{{ hostvars[groups['master'][0]]['ansible_host'] }}:6443"
        export K3S_TOKEN="{{ k3s_token }}"
        export INSTALL_K3S_VERSION={{ k3s_target_version }}
        curl -sfL {{ k3s_install_url }} | sh -
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      become: true
      register: upgrade_worker_single

    - name: Wait for K3s agent to be ready (Single Master mode)
      ansible.builtin.wait_for:
        path: /var/lib/rancher/k3s/agent/node-token
        state: present
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      become: true
      retries: 10
      delay: 10

    - name: Wait for Longhorn pods to recover after upgrade (Single Master)
      ansible.builtin.shell:
        cmd: |
          # Wait for Longhorn to recover replica pods (max 2 minutes)
          for i in {1..12}; do
            failed_replicas=$(kubectl get pods -n longhorn-system --field-selector spec.nodeName={{ inventory_hostname }} -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' | grep -c "Failed" || echo "0")
            if [ "$failed_replicas" -eq "0" ]; then
              echo "Longhorn pods recovered"
              exit 0
            fi
            sleep 10
          done
          echo "Warning: Some Longhorn pods may still be recovering"
          exit 0
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
        - longhorn_pods_before_single.stdout is defined
        - longhorn_pods_before_single.stdout != ""
      delegate_to: "{{ groups['master'][0] }}"
      register: longhorn_recovery_single
      changed_when: false
      failed_when: false

    - name: Display Longhorn recovery status (Single Master)
      ansible.builtin.debug:
        msg: |
          Longhorn recovery status for {{ inventory_hostname }}:
          {{ longhorn_recovery_single.stdout | default('No Longhorn pods found before drain') }}
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
        - longhorn_recovery_single.stdout is defined

    - name: Uncordon worker node (Single Master mode)
      ansible.builtin.command:
        cmd: kubectl uncordon {{ inventory_hostname }}
      become: true
      become_user: "{{ uid_1000_user }}"
      environment:
        KUBECONFIG: "{{ user_home_directory }}/.kube/config"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3
      delegate_to: "{{ groups['master'][0] }}"

    - name: Wait for pods to stabilize after worker upgrade (Single Master mode)
      ansible.builtin.pause:
        seconds: "{{ upgrade_wait_for_pods }}"
      when: 
        - "'worker' in group_names"
        - upgrade_needed | default(true)
        - (master_count | int) < 3

  when: 
    - "'worker' in group_names"
    - upgrade_needed | default(true)
    - (master_count | int) < 3


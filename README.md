# Ansible Role: K3S Cluster

![release](https://img.shields.io/badge/release-v1.0-blue)

Bu Ansible rolÃ¼, **K3S** tabanlÄ± Kubernetes cluster kurulumunu otomatikleÅŸtirir. HA (High Availability) ve Single Master modlarÄ±nÄ± destekler, Keepalived ile VIP yÃ¶netimi saÄŸlar ve production-ready bir Kubernetes ortamÄ± kurar.

<img src="https://k3s.io/img/k3s-logo-light.svg" alt="k3s" style="max-width: 100%;">

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#-Ã¶zellikler)
- [Ã–nkoÅŸullar](#-Ã¶nkoÅŸullar)
- [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)
- [DetaylÄ± Kurulum](#-detaylÄ±-kurulum)
- [YapÄ±landÄ±rma](#-yapÄ±landÄ±rma)
- [KullanÄ±m Ã–rnekleri](#-kullanÄ±m-Ã¶rnekleri)
- [K3s Cluster Upgrade](#-k3s-cluster-upgrade)
- [Extra Node Ekleme](#-extra-node-ekleme)
- [HA Modu KontrolÃ¼](#-ha-modu-kontrolÃ¼)
- [Pod DaÄŸÄ±lÄ±mÄ± ve Replica Stratejisi](#-pod-daÄŸÄ±lÄ±mÄ±-ve-replica-stratejisi)
- [SSL/TLS SertifikalarÄ±](#-ssltls-sertifikalarÄ±)
- [Longhorn StorageClass](#-longhorn-storageclass)
- [Troubleshooting](#-troubleshooting)

## âœ¨ Ã–zellikler

- âœ… **HA (High Availability) DesteÄŸi**: 3+ master node ile yÃ¼ksek eriÅŸilebilirlik
- âœ… **Single Master DesteÄŸi**: Tek master node ile baÅŸlayÄ±p sonradan HA'ya dÃ¶nÃ¼ÅŸtÃ¼rme
- âœ… **Keepalived VIP**: Otomatik VIP yÃ¶netimi ile master failover
- âœ… **Rolling Update**: Kesintisiz cluster upgrade
- âœ… **Idempotent**: GÃ¼venli tekrar Ã§alÄ±ÅŸtÄ±rma
- âœ… **Otomatik Values SeÃ§imi**: Master sayÄ±sÄ±na gÃ¶re otomatik Helm values seÃ§imi
- âœ… **Pod DaÄŸÄ±lÄ±mÄ±**: System pod'lar master'da, application pod'lar worker'da
- âœ… **SSL/TLS**: Cert-manager ile otomatik sertifika yÃ¶netimi
- âœ… **Monitoring**: Prometheus + Grafana + Alertmanager
- âœ… **Storage**: Longhorn ile distributed block storage
- âœ… **Load Balancer**: MetalLB ile bare metal load balancing
- âœ… **Ingress**: NGINX Ingress Controller
- âœ… **Management**: Rancher ile cluster yÃ¶netimi

## ğŸ”§ Ã–nkoÅŸullar

### 1. Ansible ve Gerekli Collection'lar

```bash
# Ansible yÃ¼klÃ¼ olmalÄ± (2.9+)
ansible --version

# Gerekli collection'Ä± yÃ¼kleyin
ansible-galaxy collection install community.general
```

### 2. SSH EriÅŸimi ve Sudo Yetkisi

TÃ¼m node'lara SSH ile eriÅŸim ve sudo yetkisi gereklidir:

```bash
# SSH key'i tÃ¼m node'lara kopyalayÄ±n
ssh-copy-id -i ~/.ssh/your_key root@192.168.1.145
ssh-copy-id -i ~/.ssh/your_key root@192.168.1.146
ssh-copy-id -i ~/.ssh/your_key root@192.168.1.147
ssh-copy-id -i ~/.ssh/your_key root@192.168.1.156
```

**Not**: EÄŸer root kullanÄ±cÄ±sÄ±nÄ±n `authorized_keys` dosyasÄ±nda gÃ¼venlik kÄ±sÄ±tlamasÄ± varsa, ÅŸu komutla kaldÄ±rabilirsiniz:

```bash
sed -i 's/^no-port-forwarding,no-agent-forwarding,no-X11-forwarding,command="echo.*exit 142" *//g' ~/.ssh/authorized_keys
```

### 3. Sistem Gereksinimleri

- **Master Node'lar**: Minimum 2 CPU, 2GB RAM (HA iÃ§in en az 3 master)
- **Worker Node'lar**: Minimum 1 CPU, 1GB RAM
- **Disk**: Minimum 20GB boÅŸ alan
- **Ä°ÅŸletim Sistemi**: Ubuntu 22.04 test edilmiÅŸtir (diÄŸer Linux daÄŸÄ±tÄ±mlarÄ± da Ã§alÄ±ÅŸabilir)

### 4. ETCD ve HA Notu

ETCD cluster'Ä± Ã§oÄŸunluk (quorum) prensibi ile Ã§alÄ±ÅŸÄ±r:
- **2 Master**: Bir master Ã§Ã¶kerse cluster yÃ¶netilemez hale gelir
- **3+ Master**: Bir master Ã§Ã¶kse bile cluster Ã§alÄ±ÅŸmaya devam eder
- **Ã–nerilen**: Production ortamlar iÃ§in en az 3 master node kullanÄ±n

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Inventory DosyasÄ±nÄ± DÃ¼zenleyin

`inventory/cluster_inventory.yml` dosyasÄ±nÄ± kendi ortamÄ±nÄ±za gÃ¶re dÃ¼zenleyin:

```yaml
all:
  children:
    master:
      hosts:
        master-1:
          ansible_host: 192.168.1.145
        master-2:
          ansible_host: 192.168.1.146
        master-3:
          ansible_host: 192.168.1.147
    worker:
      hosts:
        worker-1:
          ansible_host: 192.168.1.245
        worker-2:
          ansible_host: 192.168.1.246
```

**Not**: EÄŸer worker node istemiyorsanÄ±z, worker bÃ¶lÃ¼mÃ¼nÃ¼ yorum satÄ±rÄ± yapabilirsiniz. Sadece 3 master node ile HA cluster kurabilirsiniz.

### 2. YapÄ±landÄ±rma DeÄŸiÅŸkenlerini AyarlayÄ±n

`playbooks/roles/k3s_setup/vars/main.yml` dosyasÄ±nÄ± dÃ¼zenleyin:

```yaml
# Keepalived VIP (tÃ¼m master node'lar bu IP Ã¼zerinden baÄŸlanÄ±r)
keepalived_vip: 192.168.1.244

# K3s Versiyonu
k3s_version: "v1.32.8+k3s1"  # Ä°lk kurulum iÃ§in
k3s_upgrade_version: "v1.32.9+k3s1"  # Upgrade iÃ§in (opsiyonel)

# Hangi servisleri kurmak istediÄŸinizi belirtin
helm_install: false
traefik_uninstall: false
ingress_install: true
metallb_install: true
cert_manager_install: true
longhorn_install: true
grafana_install: true
rancher_install: true
```

### 3. Cluster'Ä± Kurun

```bash
# TÃ¼m node'larda kurulum
ansible-playbook -i inventory/cluster_inventory.yml k3s_setup.yml

# FarklÄ± SSH key kullanÄ±yorsanÄ±z
ansible-playbook -i inventory/cluster_inventory.yml k3s_setup.yml --key-file ~/.ssh/your_key
```

## ğŸ“– DetaylÄ± Kurulum

### AdÄ±m 1: Sistem Gereksinimlerinin KontrolÃ¼

Playbook otomatik olarak ÅŸunlarÄ± kontrol eder:
- CPU sayÄ±sÄ± (minimum 2 master'da)
- RAM miktarÄ± (minimum 2GB master'da)
- Disk alanÄ±
- Ä°ÅŸletim sistemi

### AdÄ±m 2: Hostname YapÄ±landÄ±rmasÄ±

Her node'un hostname'i inventory'deki isimle eÅŸleÅŸmelidir. Playbook otomatik olarak:
- Hostname'i kontrol eder
- Gerekirse deÄŸiÅŸtirir
- `/etc/hosts` dosyasÄ±nÄ± gÃ¼nceller
- Gerekirse reboot yapar

### AdÄ±m 3: NTP YapÄ±landÄ±rmasÄ±

Cluster node'larÄ± arasÄ±nda zaman senkronizasyonu kritiktir. Playbook:
- Chrony kurulumu yapar
- `time.google.com` NTP sunucusunu yapÄ±landÄ±rÄ±r
- Servisi baÅŸlatÄ±r ve enable eder

### AdÄ±m 4: Keepalived Kurulumu (Master Node'larda)

Keepalived, HA cluster'larda VIP yÃ¶netimi iÃ§in kullanÄ±lÄ±r:
- **3+ Master**: Keepalived kurulur, yapÄ±landÄ±rÄ±lÄ±r ve baÅŸlatÄ±lÄ±r
- **1-2 Master**: Keepalived kurulur ama yapÄ±landÄ±rÄ±lmaz (gelecek iÃ§in hazÄ±r)

### AdÄ±m 5: K3s Kurulumu

K3s kurulumu master sayÄ±sÄ±na gÃ¶re otomatik olarak yapÄ±lÄ±r:

**Single Master (1 master):**
```bash
k3s server --cluster-init --tls-san <keepalived_vip>
```

**HA Mode (3+ master):**
- Ä°lk master: `k3s server --cluster-init --tls-san <keepalived_vip>`
- DiÄŸer master'lar: `k3s server --server https://<keepalived_vip>:6443 --token <token>`

**Worker Node'lar:**
```bash
k3s agent --server https://<keepalived_vip>:6443 --token <token>
```

### AdÄ±m 6: Helm Kurulumu

Helm, Kubernetes paket yÃ¶neticisi olarak kurulur (eÄŸer `helm_install: true` ise).

### AdÄ±m 7: Servis KurulumlarÄ±

YapÄ±landÄ±rmaya gÃ¶re ÅŸu servisler kurulur:
- **Traefik KaldÄ±rma**: VarsayÄ±lan Traefik ingress kaldÄ±rÄ±lÄ±r
- **NGINX Ingress**: Ingress controller kurulur
- **MetalLB**: Load balancer kurulur
- **Cert-Manager**: SSL/TLS sertifika yÃ¶netimi
- **Longhorn**: Distributed block storage
- **kube-prometheus-stack**: Prometheus + Grafana + Alertmanager
- **Rancher**: Kubernetes management platform

## âš™ï¸ YapÄ±landÄ±rma

### Ana YapÄ±landÄ±rma DosyasÄ±

`playbooks/roles/k3s_setup/vars/main.yml` dosyasÄ±nda tÃ¼m yapÄ±landÄ±rma deÄŸiÅŸkenleri bulunur:

```yaml
# KullanÄ±cÄ± ve SSH
ansible_user: root
ansible_ssh_private_key_file: /home/user/.ssh/homelab

# Keepalived
keepalived_vip: 192.168.1.244
keepalived_auth_pass: P@ssw0rd123!

# K3s VersiyonlarÄ±
k3s_version: "v1.32.8+k3s1"
k3s_upgrade_version: "v1.32.9+k3s1"  # Opsiyonel

# Servis KurulumlarÄ±
helm_install: false
traefik_uninstall: false
ingress_install: true
metallb_install: true
cert_manager_install: true
longhorn_install: true
grafana_install: true
rancher_install: true
```

### Master/Worker Pod DaÄŸÄ±lÄ±mÄ±

Kurulum, **master sayÄ±sÄ±na gÃ¶re otomatik olarak** values dosyalarÄ±nÄ± seÃ§er:
- **3+ Master (HA)**: `values-ha.yml` dosyalarÄ± kullanÄ±lÄ±r
- **1 Master (Single)**: `values-single-master.yml` dosyalarÄ± kullanÄ±lÄ±r

**Pod DaÄŸÄ±lÄ±mÄ± Stratejisi:**
- **System Pod'lar** (Prometheus, Alertmanager, Cert-Manager, Ingress, MetalLB Controller): Master node'larda Ã§alÄ±ÅŸÄ±r
- **Application Pod'lar** (Grafana): Worker node'larda Ã§alÄ±ÅŸÄ±r
- **Storage Pod'lar** (Longhorn): Master preferred, worker fallback stratejisi ile Ã§alÄ±ÅŸÄ±r

## ğŸ’» KullanÄ±m Ã–rnekleri

### Cluster Kurulumu

```bash
# TÃ¼m node'larda kurulum
ansible-playbook -i inventory/cluster_inventory.yml k3s_setup.yml
```

### Cluster Upgrade

```bash
# TÃ¼m node'larda upgrade etme
ansible-playbook -i inventory/cluster_inventory.yml upgrade.yml
```

### Cluster'a Yeni Node Ekleme

```bash
# Extra node ekleme
ansible-playbook -i inventory/cluster_inventory.yml add_node.yml
```

### Cluster Durumunu Kontrol Etme

```bash
# Master node'a SSH ile baÄŸlanÄ±n
ssh root@192.168.1.145

# Node'larÄ± kontrol edin
kubectl get nodes -o wide

# Pod'larÄ± kontrol edin
kubectl get pods -A -o wide

# Servisleri kontrol edin
kubectl get svc -A
```

### Rancher'a EriÅŸim

Rancher kurulumundan sonra bootstrap ÅŸifresini almak iÃ§in:

```bash
kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}{{"\n"}}'
```

Rancher'a eriÅŸim: `https://rancher.homelab.local` (MetalLB IP'si ile `/etc/hosts` dosyanÄ±zÄ± gÃ¼ncelleyin)

### Grafana'ya EriÅŸim

Grafana admin ÅŸifresini almak iÃ§in:

```bash
kubectl get secret --namespace monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
```

Grafana'ya eriÅŸim: `https://grafana.homelab.local` (admin kullanÄ±cÄ± adÄ± ile)

## ğŸ”„ K3s Cluster Upgrade

Cluster'Ä±nÄ±zÄ± **kesintisiz** bir ÅŸekilde gÃ¼ncellemek iÃ§in rolling update stratejisi kullanÄ±lÄ±r.

### Upgrade SÃ¼reci

1. **Versiyon KontrolÃ¼**: TÃ¼m node'larÄ±n mevcut K3s versiyonlarÄ± kontrol edilir
2. **Downgrade Ã–nleme**: Mevcut versiyon hedef versiyondan yÃ¼ksekse upgrade atlanÄ±r
3. **Master Node'larÄ± GÃ¼ncelleme** (SÄ±rayla):
   - Master node'lar **tek tek** (`serial: 1`) gÃ¼ncellenir
   - Her master node iÃ§in:
     - Node **drain** edilir (pod'lar diÄŸer node'lara taÅŸÄ±nÄ±r)
     - K3s upgrade edilir
     - Node **uncordon** edilir
     - Pod'larÄ±n stabilize olmasÄ± beklenir
4. **Worker Node'larÄ± GÃ¼ncelleme** (SÄ±rayla):
   - Worker node'lar **tek tek** gÃ¼ncellenir
   - AynÄ± sÃ¼reÃ§ uygulanÄ±r
5. **Otomatik Temizlik**: `SchedulingDisabled` durumunda kalan node'lar otomatik uncordon edilir
6. **Pod Rebalancing**: Upgrade sonrasÄ± pod'lar yeniden daÄŸÄ±tÄ±lÄ±r
7. **Cluster DoÄŸrulama**: TÃ¼m node'larÄ±n Ready durumda olduÄŸu kontrol edilir

### Upgrade Ã‡alÄ±ÅŸtÄ±rma

**1. Versiyon Belirleme**: `playbooks/roles/k3s_setup/vars/main.yml` dosyasÄ±nda:

```yaml
# Ä°lk kurulum versiyonu
k3s_version: "v1.32.8+k3s1"

# Upgrade versiyonu (opsiyonel - belirtilmezse k3s_version kullanÄ±lÄ±r)
k3s_upgrade_version: "v1.32.9+k3s1"
```

**2. Upgrade Komutu**:

```bash
# TÃ¼m node'larÄ± upgrade et
ansible-playbook -i inventory/cluster_inventory.yml upgrade.yml
```

### Upgrade YapÄ±landÄ±rmasÄ±

`playbooks/roles/update_cluster/vars/main.yml` dosyasÄ±nda ayarlanabilir parametreler:

```yaml
upgrade_drain_timeout: 600          # Drain timeout (saniye) - PVC-heavy workload'lar iÃ§in artÄ±rÄ±ldÄ±
upgrade_drain_grace_period: 120     # Pod termination grace period (saniye)
upgrade_wait_for_pods: 60           # Pod stabilize bekleme sÃ¼resi (saniye)
upgrade_force: false                 # Versiyon eÅŸleÅŸse bile zorla upgrade (Ã¶nerilmez)
```

### Ã–nemli Notlar

âš ï¸ **Backup**: Upgrade Ã¶ncesi Ã¶nemli verilerinizi yedekleyin  
âš ï¸ **Test**: Production'a uygulamadan Ã¶nce test ortamÄ±nda deneyin  
âš ï¸ **Versiyon UyumluluÄŸu**: K3s versiyonlarÄ± arasÄ±nda uyumluluk kontrolÃ¼ yapÄ±n  
âš ï¸ **Etcd**: HA kurulumlarda etcd uyumluluÄŸu Ã¶nemlidir, master node'larÄ± Ã¶nce gÃ¼ncelleyin  
âš ï¸ **Rolling Update**: Node'lar sÄ±rayla gÃ¼ncellenir, cluster kesintisiz Ã§alÄ±ÅŸmaya devam eder  
âš ï¸ **Downgrade Ã–nleme**: Mevcut versiyon hedef versiyondan yÃ¼ksekse upgrade yapÄ±lmaz

### Upgrade SonrasÄ± Kontrol

```bash
# Node durumunu kontrol et
kubectl get nodes -o wide

# Node versiyonlarÄ±nÄ± kontrol et
kubectl get nodes -o custom-columns=NAME:.metadata.name,VERSION:.status.nodeInfo.kubeletVersion

# Pod durumunu kontrol et
kubectl get pods -A

# K3s versiyonunu kontrol et
kubectl version --short
```

## â• Extra Node Ekleme

Mevcut K3s cluster'Ä±nÄ±za yeni master veya worker node'lar ekleyebilirsiniz. Bu iÅŸlem **idempotent**'tir.

### Ã–nkoÅŸullar

**1. Inventory'ye Yeni Node Ekleme**: `inventory/cluster_inventory.yml` dosyasÄ±na yeni node'u ekleyin:

```yaml
all:
  children:
    master:
      hosts:
        master-1:
          ansible_host: 192.168.1.145
        master-2:
          ansible_host: 192.168.1.146
        master-3:
          ansible_host: 192.168.1.147
        master-4:  # Yeni master node
          ansible_host: 192.168.1.148
    worker:
      hosts:
        worker-1:
          ansible_host: 192.168.1.245
        worker-2:
          ansible_host: 192.168.1.246
        worker-3:  # Yeni worker node
          ansible_host: 192.168.1.247
```

**2. SSH EriÅŸimi**: Yeni node'lara SSH eriÅŸimi olmalÄ± ve sudo yetkisi bulunmalÄ±dÄ±r.

### KullanÄ±m Ã–rnekleri

**TÃ¼m yeni node'larÄ± eklemek iÃ§in:**
```bash
ansible-playbook -i inventory/cluster_inventory.yml add_node.yml
```

### Ã–zellikler

âœ… **Otomatik Hostname YapÄ±landÄ±rmasÄ±**: Yeni node'un hostname'i otomatik olarak ayarlanÄ±r (gerekirse reboot yapÄ±lÄ±r)  
âœ… **NTP YapÄ±landÄ±rmasÄ±**: Chrony kurulumu ve yapÄ±landÄ±rmasÄ± otomatik yapÄ±lÄ±r  
âœ… **Idempotent**: Zaten cluster'a eklenmiÅŸ node'larÄ± tekrar eklemez  
âœ… **Versiyon UyumluluÄŸu**: Yeni node'lar mevcut cluster versiyonu ile uyumlu kurulur  
âœ… **HA DesteÄŸi**: 3+ master node'lu HA cluster'lara master ekleyebilir  
âœ… **Single Master DesteÄŸi**: Tek master node'lu cluster'lara master ekleyerek HA'ya dÃ¶nÃ¼ÅŸtÃ¼rebilir  
âœ… **Keepalived Otomatik YapÄ±landÄ±rma**: 3+ master olduÄŸunda Keepalived otomatik yapÄ±landÄ±rÄ±lÄ±r

### Ã–nemli Notlar

âš ï¸ **Versiyon UyumluluÄŸu**: Yeni node'larÄ±n versiyonu mevcut cluster versiyonu ile uyumlu olmalÄ±dÄ±r. Versiyon `playbooks/roles/k3s_setup/vars/main.yml` dosyasÄ±ndaki `k3s_version` deÄŸiÅŸkeninden alÄ±nÄ±r.

âš ï¸ **Hostname DeÄŸiÅŸikliÄŸi**: EÄŸer node'un hostname'i inventory'deki isimle eÅŸleÅŸmiyorsa, hostname deÄŸiÅŸtirilir ve sistem reboot edilir.

âš ï¸ **Token GÃ¼venliÄŸi**: K3s token'Ä± otomatik olarak ilk master node'dan alÄ±nÄ±r.

âš ï¸ **Single Master'dan HA'ya DÃ¶nÃ¼ÅŸÃ¼m**: Ä°lk master node `--cluster-init` ile kurulmuÅŸ olmalÄ±dÄ±r. Aksi halde ek master eklenemez.

### DoÄŸrulama

Node'un baÅŸarÄ±yla eklendiÄŸini kontrol etmek iÃ§in:

```bash
kubectl get nodes -o wide
```

Yeni node'un `Ready` durumunda olduÄŸunu gÃ¶rmelisiniz.

## ğŸ” HA Modu KontrolÃ¼

Cluster'Ä±nÄ±zÄ±n HA modunda Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in:

### Komple Kontrol Komutu

```bash
echo "=== HA MODE CHECK ===" && \
echo "Master nodes:" && \
kubectl get nodes -l node-role.kubernetes.io/master --no-headers | wc -l && \
echo "ETCD nodes:" && \
kubectl get nodes -l node-role.kubernetes.io/etcd --no-headers | wc -l && \
echo "Keepalived VIP:" && \
ip -4 a show ens3 | grep 192.168.1.244 && \
echo "ETCD directory:" && \
ls -d /var/lib/rancher/k3s/server/db/etcd 2>/dev/null && \
echo "HA Mode: YES (3+ masters with etcd cluster)" || echo "HA Mode: NO"
```

### Tek Tek Kontrol

```bash
# 1. Master node sayÄ±sÄ± (3+ = HA)
kubectl get nodes -l node-role.kubernetes.io/master --no-headers | wc -l

# 2. ETCD node sayÄ±sÄ± (3+ = HA)
kubectl get nodes -l node-role.kubernetes.io/etcd --no-headers | wc -l

# 3. Keepalived VIP kontrolÃ¼
ip -4 a show ens3 | grep 192.168.1.244

# 4. ETCD cluster dizini (varsa = HA)
ls -d /var/lib/rancher/k3s/server/db/etcd

# 5. TÃ¼m master node'larÄ± listele
kubectl get nodes -l node-role.kubernetes.io/master -o wide
```

**HA Modu Kriterleri:**
- âœ… 3 veya daha fazla master node
- âœ… Her master'da ETCD Ã§alÄ±ÅŸÄ±yor
- âœ… `--cluster-init` ile baÅŸlatÄ±lmÄ±ÅŸ (etcd cluster dizini var)
- âœ… Keepalived VIP yapÄ±landÄ±rÄ±lmÄ±ÅŸ (opsiyonel ama Ã¶nerilir)

## ğŸ“Š Pod DaÄŸÄ±lÄ±mÄ± ve Replica Stratejisi

### Replica DaÄŸÄ±lÄ±mÄ±

| Component | HA (3+ Masters) | Single Master |
|-----------|----------------|---------------|
| **Ingress-Nginx** | 2 replicas | 1 replica |
| **Cert-Manager Controller** | 2 replicas | 1 replica |
| **Cert-Manager Webhook** | 3 replicas | 1 replica |
| **Cert-Manager CA Injector** | 2 replicas | 1 replica |
| **Prometheus** | 2 replicas | 1 replica |
| **Alertmanager** | 2 replicas | 1 replica |
| **Longhorn UI** | 2 replicas | 1 replica |
| **Longhorn CSI Attacher** | 3 replicas | 1 replica |
| **Longhorn CSI Provisioner** | 3 replicas | 1 replica |
| **Longhorn CSI Resizer** | 3 replicas | 1 replica |
| **Longhorn CSI Snapshotter** | 3 replicas | 1 replica |
| **Grafana** | 1 replica | 1 replica |
| **Rancher** | 2 replicas | 2 replicas |

### Pod DaÄŸÄ±lÄ±mÄ± Tablosu

| Component | Namespace | HA Replicas | Single Replicas | Node Preference |
|-----------|-----------|-------------|-----------------|-----------------|
| **Ingress-Nginx Controller** | ingress-nginx | 2 | 1 | Master |
| **MetalLB Controller** | metallb-system | 1 | 1 | Master |
| **MetalLB Speaker** | metallb-system | DaemonSet | DaemonSet | All Nodes |
| **Cert-Manager Controller** | cert-manager | 2 | 1 | Master |
| **Cert-Manager Webhook** | cert-manager | 3 | 1 | Master |
| **Cert-Manager CA Injector** | cert-manager | 2 | 1 | Master |
| **Prometheus** | monitoring | 2 | 1 | Master (preferred) |
| **Alertmanager** | monitoring | 2 | 1 | Master (preferred) |
| **Grafana** | monitoring | 1 | 1 | Worker |
| **Kube State Metrics** | monitoring | 1 | 1 | Master (preferred) |
| **Longhorn Manager** | longhorn-system | DaemonSet | DaemonSet | All Nodes |
| **Longhorn UI** | longhorn-system | 2 | 1 | Master (preferred) |
| **Longhorn CSI Components** | longhorn-system | 3 | 1 | Master (preferred) |
| **Rancher** | cattle-system | 2 | 2 | Any |

## ğŸ” SSL/TLS SertifikalarÄ±

TÃ¼m servisler iÃ§in SSL/TLS sertifikalarÄ± `cert-manager` ile otomatik olarak yÃ¶netilir. Cert dosyalarÄ± ayrÄ± dizinlerde tutulur:

### Grafana
- **Dizin**: `files/my-charts/grafana/cert/`
- **Dosyalar**:
  - `homelab.grafana.yml` - Ingress
  - `homelab-grafana-certificate.yml` - Certificate
- **Domain**: `grafana.homelab.local`

### Longhorn
- **Dizin**: `files/my-charts/longhorn/cert/`
- **Dosyalar**:
  - `homelab.longhorn.yml` - Ingress
  - `homelab-longhorn-certificate.yml` - Certificate
- **Domain**: `longhorn.homelab.local`

### Rancher
- **Dizin**: `files/my-charts/rancher/cert/`
- **Dosyalar**:
  - `rancher.homelab.yml` - Ingress
  - `rancher-homelab-certificate.yml` - Certificate
- **Domain**: `rancher.homelab.local`

### Hosts DosyasÄ± YapÄ±landÄ±rmasÄ±

Yerel eriÅŸim iÃ§in `/etc/hosts` dosyanÄ±za ÅŸu satÄ±rlarÄ± ekleyin:

```bash
# K3s Cluster Services
192.168.1.242    rancher.homelab.local
192.168.1.242    grafana.homelab.local
192.168.1.242    longhorn.homelab.local
```

**Not**: IP adresi (`192.168.1.242`) MetalLB LoadBalancer IP'sidir. Ingress Controller servisinin IP'sini kontrol etmek iÃ§in:

```bash
kubectl get svc -n ingress-nginx ingress-nginx-controller
```

## ğŸ’¾ Longhorn StorageClass

Longhorn, Kubernetes iÃ§in daÄŸÄ±tÄ±lmÄ±ÅŸ blok depolama saÄŸlar. Kurulum sÄ±rasÄ±nda otomatik olarak 6 farklÄ± StorageClass oluÅŸturulur:

### StorageClass'lar

| StorageClass | ReclaimPolicy | Replica SayÄ±sÄ± | KullanÄ±m AmacÄ± |
|-------------|---------------|---------------|----------------|
| `longhorn-retain-1` | Retain | 1 | Single Master kurulumlar iÃ§in |
| `longhorn-retain-2` | Retain | 2 | HA kurulumlar iÃ§in (Ã¶nerilen) |
| `longhorn-retain-3` | Retain | 3 | YÃ¼ksek veri gÃ¼venliÄŸi gereken durumlar |
| `longhorn-delete-1` | Delete | 1 | GeÃ§ici veriler iÃ§in |
| `longhorn-delete-2` | Delete | 2 | GeÃ§ici veriler iÃ§in (HA) |
| `longhorn-delete-3` | Delete | 3 | GeÃ§ici veriler iÃ§in (yÃ¼ksek gÃ¼venlik) |

### Mevcut PVC YapÄ±landÄ±rmasÄ±

Kurulumda kullanÄ±lan StorageClass'lar:

- **Prometheus**: `longhorn-retain-2` (HA iÃ§in 2 replica)
- **Alertmanager**: `longhorn-retain-2` (HA iÃ§in 2 replica)
- **Grafana**: `longhorn-retain-2` (HA iÃ§in 2 replica)

### Veri KalÄ±cÄ±lÄ±ÄŸÄ± ve GÃ¼venlik

âœ… **ReclaimPolicy: Retain** - PVC silinse bile volume'lar korunur, manuel temizlik gerekir  
âœ… **Pod Restart**: Veri korunur (PVC baÄŸlÄ± kalÄ±r)  
âœ… **Node Restart**: Veri korunur (Longhorn volume'lar farklÄ± node'larda replike edilir)  
âœ… **HA Kurulum**: `longhorn-retain-2` ile bir node Ã§Ã¶kse bile veri kaybÄ± olmaz

### StorageClass KontrolÃ¼

Mevcut StorageClass'larÄ± kontrol etmek iÃ§in:

```bash
kubectl get storageclass
```

PVC'leri kontrol etmek iÃ§in:

```bash
kubectl get pvc -A
```

### Ã–neriler

- **HA Kurulumlar (3+ Master)**: `longhorn-retain-2` veya `longhorn-retain-3` kullanÄ±n
- **Single Master**: `longhorn-retain-1` yeterlidir
- **Production OrtamlarÄ±**: En az 2 replica (`longhorn-retain-2`) kullanÄ±n
- **Kritik Veriler**: 3 replica (`longhorn-retain-3`) kullanÄ±n

## ğŸ”§ Troubleshooting

### Node'lar Ready Durumunda DeÄŸil

```bash
# Node durumunu kontrol et
kubectl get nodes

# Node detaylarÄ±nÄ± kontrol et
kubectl describe node <node-name>

# K3s servis durumunu kontrol et
systemctl status k3s  # Master node'larda
systemctl status k3s-agent  # Worker node'larda

# K3s loglarÄ±nÄ± kontrol et
journalctl -u k3s -n 50 --no-pager
```

### Pod'lar Ã‡alÄ±ÅŸmÄ±yor

```bash
# TÃ¼m pod'larÄ± listele
kubectl get pods -A

# Problemli pod'larÄ± kontrol et
kubectl describe pod <pod-name> -n <namespace>

# Pod loglarÄ±nÄ± kontrol et
kubectl logs <pod-name> -n <namespace>
```

### Keepalived VIP Ã‡alÄ±ÅŸmÄ±yor

```bash
# Keepalived servis durumunu kontrol et
systemctl status keepalived

# Keepalived loglarÄ±nÄ± kontrol et
journalctl -u keepalived -n 50 --no-pager

# VIP'in hangi node'da olduÄŸunu kontrol et
ip -4 a show ens3 | grep 192.168.1.244
```

### Upgrade SonrasÄ± Sorunlar

```bash
# Node'u manuel olarak uncordon et
kubectl uncordon <node-name>

# Node'u manuel olarak drain et
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# K3s servisini yeniden baÅŸlat
systemctl restart k3s  # Master node'larda
systemctl restart k3s-agent  # Worker node'larda
```

### Rancher Bootstrap Åifresi

```bash
kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}{{"\n"}}'
```

### Grafana Admin Åifresi

```bash
kubectl get secret --namespace monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
```

## ğŸ“ YapÄ±

```bash
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ collections
â”‚   â””â”€â”€ requirements.yml
â”œâ”€â”€ hosts
â”œâ”€â”€ inventory
â”‚   â””â”€â”€ cluster_inventory.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ playbooks
â”‚   â””â”€â”€ roles
â”‚       â””â”€â”€ k3s_setup
â”‚           â”œâ”€â”€ files
â”‚           â”‚   â””â”€â”€ my-charts
â”‚           â”‚       â”œâ”€â”€ cert-manager
â”‚           â”‚       â”‚   â”œâ”€â”€ selfsigned-issuer.yml
â”‚           â”‚       â”‚   â”œâ”€â”€ values-ha.yml
â”‚           â”‚       â”‚   â””â”€â”€ values-single-master.yml
â”‚           â”‚       â”œâ”€â”€ grafana
â”‚           â”‚       â”‚   â”œâ”€â”€ cert
â”‚           â”‚       â”‚   â”‚   â”œâ”€â”€ homelab-grafana-certificate.yml
â”‚           â”‚       â”‚   â”‚   â””â”€â”€ homelab.grafana.yml
â”‚           â”‚       â”‚   â”œâ”€â”€ kube-prometheus-stack-values-master-only.yml
â”‚           â”‚       â”‚   â”œâ”€â”€ kube-prometheus-stack-values-single-master.yml
â”‚           â”‚       â”‚   â””â”€â”€ kube-prometheus-stack-values.yml
â”‚           â”‚       â”œâ”€â”€ ingress
â”‚           â”‚       â”‚   â”œâ”€â”€ values-ha.yml
â”‚           â”‚       â”‚   â””â”€â”€ values-single-master.yml
â”‚           â”‚       â”œâ”€â”€ longhorn
â”‚           â”‚       â”‚   â”œâ”€â”€ cert
â”‚           â”‚       â”‚   â”‚   â”œâ”€â”€ homelab-longhorn-certificate.yml
â”‚           â”‚       â”‚   â”‚   â””â”€â”€ homelab.longhorn.yml
â”‚           â”‚       â”‚   â”œâ”€â”€ storageclass.yml
â”‚           â”‚       â”‚   â”œâ”€â”€ values-ha.yml
â”‚           â”‚       â”‚   â””â”€â”€ values-single-master.yml
â”‚           â”‚       â”œâ”€â”€ metallb
â”‚           â”‚       â”‚   â”œâ”€â”€ metallb-config.yml
â”‚           â”‚       â”‚   â”œâ”€â”€ values-ha.yml
â”‚           â”‚       â”‚   â””â”€â”€ values-single-master.yml
â”‚           â”‚       â””â”€â”€ rancher
â”‚           â”‚           â”œâ”€â”€ cert
â”‚           â”‚           â”‚   â”œâ”€â”€ rancher-homelab-certificate.yml
â”‚           â”‚           â”‚   â””â”€â”€ rancher.homelab.yml
â”‚           â”‚           â””â”€â”€ rancher-deployment.yml
â”‚           â”œâ”€â”€ handlers
â”‚           â”‚   â””â”€â”€ main.yml
â”‚           â”œâ”€â”€ meta
â”‚           â”œâ”€â”€ tasks
â”‚           â”‚   â”œâ”€â”€ 00_system_requirements.yml
â”‚           â”‚   â”œâ”€â”€ 00_traefik_uninstall.yml
â”‚           â”‚   â”œâ”€â”€ 00_wellcome.yml
â”‚           â”‚   â”œâ”€â”€ 01_configure_hostname.yml
â”‚           â”‚   â”œâ”€â”€ 02_install_keepalived.yml
â”‚           â”‚   â”œâ”€â”€ 03_install_k3s.yml
â”‚           â”‚   â”œâ”€â”€ 04_install_helm.yml
â”‚           â”‚   â”œâ”€â”€ 05_ingress_install.yml
â”‚           â”‚   â”œâ”€â”€ 06_metallb_install.yml
â”‚           â”‚   â”œâ”€â”€ 07_cert_manager_install.yml
â”‚           â”‚   â”œâ”€â”€ 08_longhorn_install.yml
â”‚           â”‚   â”œâ”€â”€ 09_grafana_install.yml
â”‚           â”‚   â”œâ”€â”€ 10_rancher_install.yml
â”‚           â”‚   â””â”€â”€ main.yml
â”‚           â”œâ”€â”€ templates
â”‚           â”‚   â”œâ”€â”€ chrony.j2
â”‚           â”‚   â”œâ”€â”€ keepalived-backup.j2
â”‚           â”‚   â”œâ”€â”€ keepalived-master.j2
â”‚           â”‚   â””â”€â”€ wellcome.j2
â”‚           â””â”€â”€ vars
â”‚               â””â”€â”€ main.yml
â”œâ”€â”€ README.md
â””â”€â”€ site.yml

22 directories, 48 files
```

---

**Not**: Bu role ÅŸu anda Ubuntu 22.04 Ã¼zerinde test edilmiÅŸtir. K3s'in resmi kurulum scripti (`curl -sfL https://get.k3s.io | sh -`) kullanÄ±ldÄ±ÄŸÄ± iÃ§in diÄŸer Linux daÄŸÄ±tÄ±mlarÄ±nda da Ã§alÄ±ÅŸmasÄ± beklenir.
